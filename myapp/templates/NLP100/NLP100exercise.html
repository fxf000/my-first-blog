{% load static %}

<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="NLP100 Exercise">
  <meta name="keywords" content="programming, python, Algorithm, Machine Learning, Natural Language Processing">
  <link rel="stylesheet" type="text/css" href="{% static "./css/style.css" %}">

  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
  <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<body>
  <div id="container">
  <header>
  <h1 id="logo"><a href="{% url 'index' %}">
    <img src="{% static './images/logo.png' %}" alt="">
  </a></h1>
  <iframe src="{% static './countdown/countdown.html' %}" frameBorder="0" scrolling="no"></iframe>
  <nav id="menubar">
  <ul>
  <li><a href="{% url 'record' %}">RECORD</a></li>
  <li><a href="{% url 'category' %}">CATEGORY</a></li>
  <li><a href="{% url 'about_me' %}">ABOUT ME</a></li>
  <li><a href="{% url 'link' %}">LINK</a></li>
  <li><a href="{% url 'index' %}#CONTACT">CONTACT</a></li>
  </ul>
  </nav>
  </header>
  <div id="contents">
  <h3 id="NLP100Exercise">
    <a class="no-underline" href="https://nlp100.github.io/ja/" target="_blank" rel="noopener noreferrer">
    <b><font color="#000" face="Comic Sans MS">NLP100 Exercise 2020</font></b>
    </a>
  </h3>
  <section>
    <p>
    I, who haven't studied information field, solved
    <a href="https://nlp100.github.io/ja/">NLP100 Exercise</a>
    problems with python3 as
    part of learning the basic of Natural Language Processing.
    NLP100 Exercise is a bootcamp designed for beginner who don't know well programming,
    data analysis and research around NLP.
    </p>
    <p>
    I would like to post the code that solved the NLP100 Exercise problems by myself,
    referring to various sites.
    Then, if you have any technical advice and so on, please feel free to contact me.
    </p>
  </section>
  <section>
    <ol class="wrap-css-selector">
    <div style="padding: 10px; margin-bottom: 10px; border: 2px solid #333333; width:90%; background-color: #dcdcdc;">
      <b><font size="3" color="#000">Contnts</font></b>
      <p style="margin: -5px 20px -5px 40px;">
    <ol>
      <li>Warm-up
        </br>
        Train important topics of programming to handle strings and texts in this section.
        <ol>
          <li><a href="{% url 'blog_202005' %}#05/11/2020">Reversed string</a></li>
          <li><a href="{% url 'blog_202005' %}#05/11/2020">"「パタトクカシーー」"</a></li>
          <li><a href="{% url 'blog_202005' %}#05/11/2020">"「パトカー」＋「タクシー」＝「パタトクカシーー」"</a></li>
          <li><a href="{% url 'blog_202005' %}#05/11/2020">Pi</a></li>
          <li><a href="{% url 'blog_202005' %}#05/12/2020">Atomic symbols</a></li>
          <li><a href="{% url 'blog_202005' %}#05/15/2020">N-gram</a></li>
          <li><a href="{% url 'blog_202005' %}#05/23/2020">Set</a></li>
          <li><a href="{% url 'blog_202005' %}#05/28/2020">Template-based sentence generation</a></li>
          <li><a href="{% url 'blog_202005' %}#05/29/2020">Cipher text</a></li>
          <li><a href="{% url 'blog_202005' %}#05/29/2020">Typoglycemia</a></li>
        </ol>
      </li>
      <li>UNIX commands
        </br>
        Experience UNIX commands convenient for such as data analysis.
        <ol>
          <li><a href="{% url 'blog_202006' %}#06/02/2020">Line count</a></li>
          <li><a href="{% url 'blog_202006' %}#06/12/2020">Replace tabs into spaces</a></li>
          <li><a href="{% url 'blog_202006' %}#06/12/2020">Col1.txt from the first column, col2.txt from the second column</a></li>
          <li><a href="{% url 'blog_202006' %}#06/16/2020">Merging col1.txt and col2.txt</a></li>
          <li><a href="{% url 'blog_202006' %}#06/16/2020">First N lines</a></li>
          <li><a href="{% url 'blog_202006' %}#06/16/2020">Last N lines</a></li>
          <li><a href="{% url 'blog_202006' %}#06/16/2020">Split a file into N pieces</a></li>
          <li><a href="{% url 'blog_202006' %}#06/26/2020">Distinct strings in the first column</a></li>
          <li><a href="{% url 'blog_202006' %}#06/26/2020">Sort lines in descending order of the third column</a></li>
          <li><a href="{% url 'blog_202006' %}#06/26/2020">Frequency of a string in the first column in descending order</a></li>
        </ol>
      </li>
      <li>Regular Expression
        </br>
        Learn about regular expressions to get the necessary information and knowledge.
        <ol>
          <li><a href="{% url 'blog_202007' %}#07/27/2020">Read JSON documents</a></li>
          <li><a href="{% url 'blog_202007' %}#07/27/2020">Extract lines with category names</a></li>
          <li><a href="{% url 'blog_202007' %}#07/27/2020">Extract category names</a></li>
          <li><a href="{% url 'blog_202007' %}#07/27/2020">Section structure</a></li>
          <li><a href="{% url 'blog_202008' %}#08/04/2020">Extract media references</a></li>
          <li><a href="{% url 'blog_202008' %}#08/04/2020">Extract infobox</a></li>
          <li><a href="{% url 'blog_202008' %}#08/12/2020">Remove emphasis markups</a></li>
          <li><a href="{% url 'blog_202008' %}#08/19/2020">Remove internal links</a></li>
          <li>Remove MediaWiki markups &#8658; Skipped!</li>
          <li><a href="{% url 'blog_202008' %}#08/28/2020">Obtain the url of the country flag</a></li>
        </ol>
      </li>
      <li>POS tagging
        </br>
        Obtain statistic values of the word in the novel, "吾輩は猫である",
        by applying a part-of-speech tagger.
        <ol>
          <li><a >Reading the result of part-of-speech tagging</a></li>
          <li><a >Verbs</a></li>
          <li><a >Base forms of verbs</a></li>
          <li><a >「AのB」</a></li>
          <li><a >Consecutive nouns</a></li>
          <li><a >Frequency of words</a></li>
          <li><a >Top-10 frequent words</a></li>
          <li><a >Top-10 words co-occurring with 「猫」</a></li>
          <li><a >Histogram</a></li>
          <li><a >Zipf’s law</a></li>
        </ol>
      </li>
      <li>Syntactic parsing
        </br>
        Manipulate dependency trees and perform analysis by applying a dependensy parser to the novel,
        "吾輩は猫である".
        <ol>
          <li><a >Read the syntactic parse result (morpheme)</a></li>
          <li><a >Read the syntactic parse result (phrase, dependency)</a></li>
          <li><a >Display of the phrase of modifier and modifiee</a></li>
          <li><a >Extract all pairs that the phrase containing nouns modifies the phrase containing verbs</a></li>
          <li><a >Visualize dependency trees</a></li>
          <li><a >Extract verb case patterns</a></li>
          <li><a >Extract verb case frame information</a></li>
          <li><a >Mining verb syntax</a></li>
          <li><a >Extract paths from the root to nouns</a></li>
          <li><a >Extract dependency paths between nouns</a></li>
        </ol>
      </li>
      <li>Machine learning
        </br>
        Build a document classifier based on machine learning and
        learn the evaluation methodology for machine learning.
        <ol>
          <li><a >Download and preprocess dataset</a></li>
          <li><a >Feature extraction</a></li>
          <li><a >Training</a></li>
          <li><a >Prediction</a></li>
          <li><a >Accuracy score</a></li>
          <li><a >Making confusion matrix</a></li>
          <li><a >Precision, recall and F1 score</a></li>
          <li><a >Confirmation of feature weights</a></li>
          <li><a >Change regularization parameter</a></li>
          <li><a >Hyper-parameter tuning</a></li>
        </ol>
      </li>
      <li>Word embeddings
        </br>
        Learn the usage of word embeddings throuch such as similarity caluculation of words
        and word analogy. In addition, experience clustering and visualization of word embeddings.
        <ol>
          <li><a >Load and display word vectors</a></li>
          <li><a >Word similarity</a></li>
          <li><a >Top-10 most similar words</a></li>
          <li><a >Analogy based on the additive composition</a></li>
          <li><a >Analogy data experiment</a></li>
          <li><a >Accuracy score on the analogy task</a></li>
          <li><a >Evaluation on WordSimilarity-353</a></li>
          <li><a >k-means clustering</a></li>
          <li><a >Ward’s method clustering</a></li>
          <li><a >t-SNE Visualization</a></li>
        </ol>
      </li>
      <li>Neural networks
        </br>
        Learn the usage of a deep-learning framework, and implement a document classifier
        based on Neural Network models.
        <ol>
          <li><a >Generating features through word vector summation</a></li>
          <li><a >Prediction of single layer Neural Network</a></li>
          <li><a >Calculating loss and gradients</a></li>
          <li><a >Learning with Stochastic Gradient Descent (SGD)</a></li>
          <li><a >Measuring accuracy</a></li>
          <li><a >Plotting loss and accuracy</a></li>
          <li><a >Checkpoints</a></li>
          <li><a >Mini-batches</a></li>
          <li><a >Training on a GPU</a></li>
          <li><a >Multilayer Neural Networks</a></li>
        </ol>
      </li>
      <li>RNN and CNN
        </br>
        Implemrnt Recurrent Neural Networks (RNNs) and
        Convolutional Neural Networks (CNNs) by using a deep-learning framework.
        <ol>
          <li><a >Turning words into numeric IDs</a></li>
          <li><a >Prediction with an RNN</a></li>
          <li><a >Training with Stochastic Gradient Descent (SGD)</a></li>
          <li><a >Mini-batch Training on GPU</a></li>
          <li><a >Add pretrained word embeddings</a></li>
          <li><a >Bi-directional RNN and multi-layer RNN</a></li>
          <li><a >Convolutional Neural Networks (CNN)Permalink</a></li>
          <li><a >CNN Learning via Stochastic Gradient Descent</a></li>
          <li><a >Hyper-parameter tuning</a></li>
          <li><a >Transfer Learning from a pretrained language model</a></li>
        </ol>
      </li>
      <li>Machine translation
        </br>
        Build a neural machine translation by using exsisting tools.
        <ol>
          <li><a >Data preprocessing</a></li>
          <li><a >Training the machine translation model</a></li>
          <li><a >Applying the learned machine translation model</a></li>
          <li><a >BLEU score measurement</a></li>
          <li><a >Beam search</a></li>
          <li><a >Subword</a></li>
          <li><a >Plotting the learning curve</a></li>
          <li><a >Hyper-parameter search</a></li>
          <li><a >Domain adaptation</a></li>
          <li><a >Building translation server</a></li>
        </ol>
      </li>
    </ol>
    </ol>
    </div>
  </section>
  </div>
  <!--/contents-->
  <footer>
  <small>Copyright&copy; <a href="{% url 'index' %}">DAILY RECORD</a> All Rights Reserved.</small>
  <span class="pr">《<a href="http://template-party.com/" target="_blank">Web Design:Template-Party</a>》</span>
  </footer>
  </div>
  <!--/container-->
</body>
</html>
